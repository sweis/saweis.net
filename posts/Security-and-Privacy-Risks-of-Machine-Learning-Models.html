<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Security &amp; Privacy Risks of Machine Learning Models</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Security &amp; Privacy Risks of Machine Learning Models</h1>
</header>
<section data-field="subtitle" class="p-summary">
This posts talks about three security and privacy risks of machine learning models: poisoning attacks, evasion attacks, and unintended…
</section>
<section data-field="body" class="e-content">
<section name="12f9" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="4ee3" id="4ee3" class="graf graf--h3 graf--leading graf--title">Security &amp; Privacy Risks of Machine Learning Models</h3><p name="7e2e" id="7e2e" class="graf graf--p graf-after--h3">This posts talks about three security and privacy risks of machine learning models: poisoning attacks, evasion attacks, and unintended memorization. For an in-depth survey, see <a href="https://arxiv.org/abs/1811.01134" data-href="https://arxiv.org/abs/1811.01134" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">“A Marauder’s Map of Security and Privacy in Machine Learning”</a>.</p><h3 name="ab99" id="ab99" class="graf graf--h3 graf-after--p">Background on Machine Learning Models</h3><figure name="cde0" id="cde0" class="graf graf--figure graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 221px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 31.6%;"></div><img class="graf-image" data-image-id="1*mqFMBIs4gsECYL5eXyOcKw.png" data-width="1892" data-height="598" src="https://cdn-images-1.medium.com/max/800/1*mqFMBIs4gsECYL5eXyOcKw.png"></div><figcaption class="imageCaption">The training phase takes a set of input training data, applies a learning process, and outputs a model.</figcaption></figure><p name="063f" id="063f" class="graf graf--p graf-after--figure">In an attempt to distill an entire field into a few sentences, machine learning generally takes a set of training data, applies a learning process, and outputs a model. The “learning process” is where most of innovation and complexity of the field lies. There are many <a href="https://see.stanford.edu/course/cs229" data-href="https://see.stanford.edu/course/cs229" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">introductory courses online</a> for more details.</p><figure name="baf7" id="baf7" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 209px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 29.799999999999997%;"></div><img class="graf-image" data-image-id="1*wXB-s2imSQcRcWm-or0XUA.png" data-width="1878" data-height="560" src="https://cdn-images-1.medium.com/max/800/1*wXB-s2imSQcRcWm-or0XUA.png"></div><figcaption class="imageCaption">The prediction phase applies the model to real data in order to make a prediction.</figcaption></figure><p name="c782" id="c782" class="graf graf--p graf-after--figure">The model itself is what does useful work. It can be applied to real data and make predictions. For instance, the model may take images and predict whether an animal is pictured, i.e. an animal <em class="markup--em markup--p-em">classifier</em>. Or, a model may perform a <em class="markup--em markup--p-em">regression</em> and forecast some continuous value.</p><figure name="363c" id="363c" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 306px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 43.8%;"></div><img class="graf-image" data-image-id="1*wlzDIbjqKKUbhVEBykSVIg.jpeg" data-width="800" data-height="350" src="https://cdn-images-1.medium.com/max/800/1*wlzDIbjqKKUbhVEBykSVIg.jpeg"></div><figcaption class="imageCaption">A machine learning model could be trained to classify whether images are chihuahuas or blueberry muffins.</figcaption></figure><p name="d6b4" id="d6b4" class="graf graf--p graf-after--figure">For a layperson, you may think of the model as a computer program that predicts whatever you trained it for. However, machine learning models differ from intentionally designed computer programs in a few ways worth mentioning:</p><ul class="postList"><li name="87cb" id="87cb" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Models are not intentionally designed.</strong> They are derived from randomized sets of training data. They are sensitive to noise and bad training input. They may differ in behavior, even if trained from the same set.</li><li name="f635" id="f635" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Models can learn weird and unintended correlations</strong>, rather than what you wanted to train them on. There are many <a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml" data-href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">anecdotes about unintended behavior</a> that have been collected.</li><li name="c13d" id="c13d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Models can “cheat”</strong>. They may overfit data and effectively memorize specific instances of training data. Keep this in mind later.</li></ul><h3 name="10a9" id="10a9" class="graf graf--h3 graf-after--li">Risks against Machine Learning Models</h3><p name="e3e4" id="e3e4" class="graf graf--p graf-after--h3">The machine learning models may be provided as as services or shipped in mobile apps. This exposes models to adversaries on a few places: training, prediction, and from the model itself.</p><h4 name="8986" id="8986" class="graf graf--h4 graf-after--p">Poisoning Attacks</h4><p name="b0dd" id="b0dd" class="graf graf--p graf-after--h4">Poisoning attacks in machine learning are when an adversary injects malicious data during the training phase with the goal of controlling how the model will behave in practice. Recall that models are not intentionally designed, so they make no distinction between “good” and “bad” data. Whatever you input to a model, it will learn.</p><figure name="9f2e" id="9f2e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 253px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 36.1%;"></div><img class="graf-image" data-image-id="1*W1fNFRYSkEZi_TW23MeZUA.png" data-width="1662" data-height="600" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*W1fNFRYSkEZi_TW23MeZUA.png"></div><figcaption class="imageCaption">An adversary able to alter training data can control how a model behaves.</figcaption></figure><p name="3082" id="3082" class="graf graf--p graf-after--figure">Microsoft learned first hand of poisoning attacks when it released <a href="https://en.wikipedia.org/wiki/Tay_%28bot%29" data-href="https://en.wikipedia.org/wiki/Tay_(bot)" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Tay</a>, which was a Twitter chatbot that was trained by real interactions with people. Microsoft allowed Tay to be trained more or less real time on unfiltered tweets from Twitter users. Predictably, it took less from a day for Tay to transform from <a href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist" data-href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">naive friendliness to full blown racist</a>.</p><figure name="d9d2" id="d9d2" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 190px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 27.200000000000003%;"></div><img class="graf-image" data-image-id="1*WF5nsY5gitmpLdnWvVoxyQ.png" data-width="1958" data-height="532" src="https://cdn-images-1.medium.com/max/800/1*WF5nsY5gitmpLdnWvVoxyQ.png"></div><figcaption class="imageCaption">Microsoft learns the perils of machine learning poisoning attacks.</figcaption></figure><p name="0a94" id="0a94" class="graf graf--p graf-after--figure">The moral of the story is that if you train your machine learning model on bad data, you are going to get a bad model. You need to sanitize your training data — but in a way that does not bias the data and skew the accuracy of the predictions.</p><p name="c3c1" id="c3c1" class="graf graf--p graf-after--p">Further reading about poisoning attacks:</p><ul class="postList"><li name="e5d9" id="e5d9" class="graf graf--li graf--startsWithDoubleQuote graf-after--p"><a href="https://arxiv.org/abs/1804.07933" data-href="https://arxiv.org/abs/1804.07933" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">“Is feature selection secure against training data poisoning?”</a></li><li name="1725" id="1725" class="graf graf--li graf--startsWithDoubleQuote graf-after--li"><a href="http://pages.cs.wisc.edu/~jerryzhu/machineteaching/pub/Mei2015Machine.pdf" data-href="http://pages.cs.wisc.edu/~jerryzhu/machineteaching/pub/Mei2015Machine.pdf" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">“Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners”</a></li></ul><h4 name="e48a" id="e48a" class="graf graf--h4 graf-after--li">Evasion Attacks</h4><p name="71e6" id="71e6" class="graf graf--p graf-after--h4">Evasion attacks occur at the prediction stage and are when an adversary has crafted an <em class="markup--em markup--p-em">adversarial example </em>which will be inaccurately classified. For example, an adversary may tweak a fraudulent transaction so that it is improperly classified as a legitimate transaction.</p><figure name="b55d" id="b55d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 206px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 29.5%;"></div><img class="graf-image" data-image-id="1*dvkRhVBYPTsKg7_xia0_ZA.png" data-width="1690" data-height="498" src="https://cdn-images-1.medium.com/max/800/1*dvkRhVBYPTsKg7_xia0_ZA.png"></div></figure><p name="6616" id="6616" class="graf graf--p graf-after--figure">Crafting adversarial examples is fairly easy in practice — often involving adding a small amount of noise. A good example case is from <a href="https://arxiv.org/abs/1412.6572" data-href="https://arxiv.org/abs/1412.6572" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">“Explaining and Harnessing Adversarial Examples”</a>, which shows how to perturb an image of, say, a panda bear, so that a machine learning model will classify it as a gibbon .</p><figure name="79d9" id="79d9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 273px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 39%;"></div><img class="graf-image" data-image-id="1*24f5-fbGPzgDv1Rhpkg0TQ.png" data-width="2254" data-height="880" src="https://cdn-images-1.medium.com/max/800/1*24f5-fbGPzgDv1Rhpkg0TQ.png"></div><figcaption class="imageCaption">A demonstration of fast adversarial example generation. Image from <a href="http://arxiv.org/abs/1412.6572" data-href="http://arxiv.org/abs/1412.6572" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Explaining and Harnessing Adversarial Examples</a> by Goodfellow et al.</figcaption></figure><p name="3d7a" id="3d7a" class="graf graf--p graf-after--figure">There are not effective solutions to evasion attacks today and <em class="markup--em markup--p-em">adversarial robustness</em> is an area of open research. Many detection techniques <a href="https://arxiv.org/abs/1705.07263" data-href="https://arxiv.org/abs/1705.07263" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">have been found ineffective</a>.</p><p name="be1d" id="be1d" class="graf graf--p graf-after--p">My opinion is that evasion attacks generally exploit models learning weird and unintended correlations. You may be able to find some insignificant feature that the model is using which can allow you to craft evasive inputs.</p><p name="57d1" id="57d1" class="graf graf--p graf-after--p">Further reading about evasion attacks:</p><ul class="postList"><li name="bb52" id="bb52" class="graf graf--li graf--startsWithDoubleQuote graf-after--p"><a href="https://openai.com/blog/adversarial-example-research/" data-href="https://openai.com/blog/adversarial-example-research/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">“Attacking Machine Learning with Adversarial Examples”</a></li><li name="1a63" id="1a63" class="graf graf--li graf--startsWithDoubleQuote graf-after--li"><a href="https://arxiv.org/abs/1412.6572" data-href="https://arxiv.org/abs/1412.6572" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">“Explaining and Harnessing Adversarial Examples”</a></li><li name="46e3" id="46e3" class="graf graf--li graf--startsWithDoubleQuote graf-after--li"><a href="https://arxiv.org/abs/1607.02533" data-href="https://arxiv.org/abs/1607.02533" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">“Adversarial examples in the physical world”</a></li><li name="ba54" id="ba54" class="graf graf--li graf--startsWithDoubleQuote graf-after--li"><a href="https://arxiv.org/abs/1602.02697" data-href="https://arxiv.org/abs/1602.02697" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">“Practical Black-Box Attacks against Machine Learning”</a></li><li name="0391" id="0391" class="graf graf--li graf--startsWithDoubleQuote graf-after--li"><a href="https://papers.nips.cc/paper/7647-adversarial-examples-that-fool-both-computer-vision-and-time-limited-humans.pdf" data-href="https://papers.nips.cc/paper/7647-adversarial-examples-that-fool-both-computer-vision-and-time-limited-humans.pdf" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">“Adversarial Examples that Fool both Computer Vision and Time-Limited Humans”</a></li></ul><h4 name="d412" id="d412" class="graf graf--h4 graf-after--li">Unintentional Memorization</h4><p name="7941" id="7941" class="graf graf--p graf-after--h4">As mentioned, machine learning models may “cheat” and memorize training data. What this means is that the model can encode specific input instances within its own parameters. Besides general overfitting, the most common case this would happen is if there are outlier samples.</p><p name="0d97" id="0d97" class="graf graf--p graf-after--p">For example, suppose a machine learning model is classifying types of animals based on their properties. It might have some equivalent to <em class="markup--em markup--p-em">“If it lays eggs, it is not a mammal unless it is a duck-billed platypus”</em>. If you can tell that a model has this rule, then you know a duck-billed platypus was part of its training set.</p><figure name="13b0" id="13b0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 212px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 30.3%;"></div><img class="graf-image" data-image-id="1*7NNA35V5C_9fp2GEgHZ-nA.png" data-width="1884" data-height="570" src="https://cdn-images-1.medium.com/max/800/1*7NNA35V5C_9fp2GEgHZ-nA.png"></div><figcaption class="imageCaption">This illustrates how an overfit model may encode specific input instances that may later be extracted.</figcaption></figure><p name="2794" id="2794" class="graf graf--p graf-after--figure">It is practical to extract private training data from machine learning models. One example <a href="https://arxiv.org/abs/1802.08232" data-href="https://arxiv.org/abs/1802.08232" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">extracted credit card numbers and social security numbers</a> from machine learning models trained on a public data set; in this case Enron’s emails.</p><p name="0086" id="0086" class="graf graf--p graf-after--p">Fortunately, unintentional memorization is one of the risks which we have an effective countermeasures: <em class="markup--em markup--p-em">differential privacy</em>. By injecting noise during either training or prediction, you can account for the privacy lost on each query to a model. Google’s <a href="https://github.com/tensorflow/privacy" data-href="https://github.com/tensorflow/privacy" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow Privacy</a> is an example of using these techniques. The tradeoff with differential privacy is that it may sacrifice accuracy.</p><p name="f04b" id="f04b" class="graf graf--p graf-after--p">Further reading on unintended memorization:</p><ul class="postList"><li name="6a26" id="6a26" class="graf graf--li graf--startsWithDoubleQuote graf-after--p"><a href="https://arxiv.org/abs/1802.08232" data-href="https://arxiv.org/abs/1802.08232" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">“The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks”</a></li><li name="3dbd" id="3dbd" class="graf graf--li graf--startsWithDoubleQuote graf-after--li"><a href="https://arxiv.org/abs/1709.07886" data-href="https://arxiv.org/abs/1709.07886" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">“Machine Learning Models that Remember Too Much”</a></li><li name="c67d" id="c67d" class="graf graf--li graf--startsWithDoubleQuote graf-after--li"><a href="https://arxiv.org/abs/1607.00133" data-href="https://arxiv.org/abs/1607.00133" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">“Deep learning with differential privacy”</a></li><li name="9421" id="9421" class="graf graf--li graf--startsWithDoubleQuote graf-after--li graf--trailing"><a href="https://arxiv.org/abs/1708.08022" data-href="https://arxiv.org/abs/1708.08022" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">“On the Protection of Private Information in Machine Learning Systems: Two Recent Approaches”</a></li></ul></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@sweis" class="p-author h-card">Steve Weis</a> on <a href="https://medium.com/p/cd0a44ac22b9"><time class="dt-published" datetime="2019-04-05T18:45:45.949Z">April 5, 2019</time></a>.</p><p><a href="https://medium.com/@sweis/security-privacy-risks-of-machine-learning-models-cd0a44ac22b9" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on January 7, 2020.</p></footer></article></body></html>